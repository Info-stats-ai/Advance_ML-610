{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyNo8MOQo9P5IcaImSUbQdby",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Info-stats-ai/Advance_ML-610/blob/master/Custom__Finetuning_Training_Dreambooth.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine tuning Methods:\n",
        "### Additonal Trianing :\n",
        "- training a base model with an additional dataset.\n",
        "### Dreamboot :\n",
        "- developed by google ,a  technique for injecting custom subjects into model, DUe to its architecture , it is possible to achieve great results using only 3/5 custom images\n",
        "\n",
        "### Textual inevrsion :\n",
        " - it injects a custom subject into the model wiht just few examples, all traning is done only in the embedding neural network."
      ],
      "metadata": {
        "id": "_gk8znvyMxzz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dreambooth\n",
        "- method to fine tune text to image models like stable diffusion.\n",
        "- It solves two problems when inseting the object into the model : overfitting(as dataset is too small) and langauge drift.\n",
        "- Using the rare words\n",
        "- Preservation of the class: to preserve the meaningo of the class , the model is adjusted ins uch a way that the subject is injected while the generation of the class image is preserved.\n",
        "## It requires 3  cthings for training:\n",
        "- 1 Unique Identifier\n",
        "- 2 Class name\n",
        "- 3 Images of the subject to be inserted\n",
        "## In our Implementation we are going to using the face of a person , class is person\n"
      ],
      "metadata": {
        "id": "YC66HByIOF-w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vzY_ZPndJQ42"
      },
      "outputs": [],
      "source": [
        "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/examples/dreambooth/train_dreambooth.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/scripts/convert_diffusers_to_original_stable_diffusion.py"
      ],
      "metadata": {
        "id": "5XHFzCROQgbF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -qq git+https://github.com/ShivamShrirao/diffusers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mQi2Z2AQrg7",
        "outputId": "d84a7552-6f7a-4a9f-ec2b-3bddbb4d73fb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for diffusers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q -U -pre triton"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkaW7HypQ3ps",
        "outputId": "00b75dff-5c9a-4968-a4b3-45caba13937a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Usage:   \n",
            "  pip3 install [options] <requirement specifier> [package-index-options] ...\n",
            "  pip3 install [options] -r <requirements file> [package-index-options] ...\n",
            "  pip3 install [options] [-e] <vcs project url> ...\n",
            "  pip3 install [options] [-e] <local project path> ...\n",
            "  pip3 install [options] <archive url/path> ...\n",
            "\n",
            "no such option: -p\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q accelerate transformers ftfy bitsandbytes gradio natsort safetensors xformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bepPOeMJQ_1U",
        "outputId": "c4f24699-949c-4af4-be56-c2abc9af5b32"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_sd = \"runyaml/stable-diffusion-v1-5\""
      ],
      "metadata": {
        "id": "lzGwJzDYRJlK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir =\"/content/stable_diffusion_weights/zwx\""
      ],
      "metadata": {
        "id": "Z_KlssDkRgw8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir -p $output_dir"
      ],
      "metadata": {
        "id": "vFdOylkKRt2u"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip /content/stable_diffusion_weights/zwx/dataset_dave-20250917T185257Z-1-001.zip"
      ],
      "metadata": {
        "id": "Pe5zJYfKR6gx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "765307eb-53e2-4003-8cca-4e35b9ad5776"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/stable_diffusion_weights/zwx/dataset_dave-20250917T185257Z-1-001.zip\n",
            "  inflating: dataset_dave/dave_09.jpg  \n",
            "  inflating: dataset_dave/dave_06.jpg  \n",
            "  inflating: dataset_dave/dave_08.jpg  \n",
            "  inflating: dataset_dave/dave_02.jpg  \n",
            "  inflating: dataset_dave/dave_04.jpg  \n",
            "  inflating: dataset_dave/dave_10.jpg  \n",
            "  inflating: dataset_dave/dave_07.jpg  \n",
            "  inflating: dataset_dave/dave_03.jpg  \n",
            "  inflating: dataset_dave/dave_01.jpg  \n",
            "  inflating: dataset_dave/dave_05.jpg  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training :\n",
        "1) unique Identifier\n",
        "2) Class Name\n",
        "3) Images\n",
        "- class prompt: photo of [class name]\n",
        "- instance prompt will be a zwx person"
      ],
      "metadata": {
        "id": "pgoKOHTDT_U5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Directories"
      ],
      "metadata": {
        "id": "hngBaA3tW7Wo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "concepts_list = [{\n",
        "    \"instance_prompt\":     \"photo of zwx\",\n",
        "    \"class_prompt\":        \"photo of a person\",\n",
        "    \"instance_data_dir\":   \"/content/dataset_dave\",\n",
        "    \"class_data_dir\":      \"/content/dataset_dave\"}\n",
        "]"
      ],
      "metadata": {
        "id": "ZPHsXZtCT-Nc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import random"
      ],
      "metadata": {
        "id": "qwwVBqxzVNbJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for c in concepts_list:\n",
        "  os.makedirs(c[\"instance_data_dir\"], exist_ok=True)\n",
        "  os.makedirs(c[\"class_data_dir\"], exist_ok=True)"
      ],
      "metadata": {
        "id": "-Ro0XZvVVUVG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"concepts_list.json\", \"w\") as f:\n",
        "    json.dump(concepts_list, f, indent=4)\n",
        "    # created he json file to provide the it to the model for the training\n",
        "    \"\"\"provide the concepts_list.json file because it acts as a configuration file for the training script (train_dreambooth.py). This JSON file contains crucial information that the script needs to know, such as:\n",
        "\n",
        "instance_prompt: The unique phrase associated with the subject you want to train (e.g., \"photo of zwx\").\n",
        "class_prompt: The general category of the subject (e.g., \"photo of a person\").\n",
        "instance_data_dir: The directory containing the images of your specific subject.\n",
        "class_data_dir: The directory containing images of the general class.\n",
        "The training script will read this JSON file to understand what to train and where to find the necessary data. It then uses this information to fine-tune the Stable Diffusion model.\n",
        "\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "FeAObmd5Vc3G"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# in your notebook / shell\n",
        "! pip install --upgrade pip\n",
        "! pip install --upgrade diffusers huggingface_hub\n",
        "# then restart the Python kernel / runtime\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VKBIPobl1uu",
        "outputId": "1cb10e31-9311-4c17-83b1-df7ae560a7c5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "Downloading pip-25.2-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-25.2\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.12/dist-packages (0.15.0.dev0)\n",
            "Collecting diffusers\n",
            "  Downloading diffusers-0.35.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.34.4)\n",
            "Collecting huggingface_hub\n",
            "  Downloading huggingface_hub-0.35.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers) (8.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from diffusers) (3.19.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from diffusers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from diffusers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from diffusers) (2.32.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from diffusers) (0.6.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from diffusers) (11.3.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.1.9)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers) (3.23.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (2025.8.3)\n",
            "Downloading diffusers-0.35.1-py3-none-any.whl (4.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m109.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.35.0-py3-none-any.whl (563 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m563.4/563.4 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: huggingface_hub, diffusers\n",
            "\u001b[2K  Attempting uninstall: huggingface_hub\n",
            "\u001b[2K    Found existing installation: huggingface-hub 0.34.4\n",
            "\u001b[2K    Uninstalling huggingface-hub-0.34.4:\n",
            "\u001b[2K      Successfully uninstalled huggingface-hub-0.34.4\n",
            "\u001b[2K  Attempting uninstall: diffusers\n",
            "\u001b[2K    Found existing installation: diffusers 0.15.0.dev0\n",
            "\u001b[2K    Uninstalling diffusers-0.15.0.dev0:\n",
            "\u001b[2K      Successfully uninstalled diffusers-0.15.0.dev0\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [diffusers]\n",
            "\u001b[1A\u001b[2KSuccessfully installed diffusers-0.35.1 huggingface_hub-0.35.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameters:"
      ],
      "metadata": {
        "id": "KCFcFXGoW32g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_images = 10\n",
        "num_class_images = num_images*12\n",
        "max_num_steps =num_images*80 # default equation\n",
        "learning_rate = 1e-6\n",
        "lr_warmup_steps = int(max_num_steps/10)\n",
        "print(num_images, num_class_images, max_num_steps, learning_rate, lr_warmup_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnvcybHLWCno",
        "outputId": "3c72008a-bd29-4b38-e86a-3d0636444aa8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 120 800 1e-06 80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -c \"from diffusers import StableDiffusionPipeline; print('diffusers import OK')\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dO2YCm8qmVnR",
        "outputId": "e134e739-1e69-4d0b-89c8-f203ec770251"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-09-18 06:59:06.758038: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1758178746.778610    5657 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1758178746.784973    5657 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1758178746.800789    5657 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758178746.800813    5657 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758178746.800816    5657 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758178746.800819    5657 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "diffusers import OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D5QUWzE8mVFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "! python3 train_dreambooth.py \\"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKYwORIuWtMR",
        "outputId": "0308d097-954d-411c-acb0-b0027439f86d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-09-18 07:00:18.967511: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1758178818.986973    6044 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1758178818.992960    6044 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1758178819.008209    6044 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758178819.008235    6044 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758178819.008238    6044 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758178819.008240    6044 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-09-18 07:00:19.012732: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "usage: train_dreambooth.py [-h] --pretrained_model_name_or_path\n",
            "                           PRETRAINED_MODEL_NAME_OR_PATH\n",
            "                           [--pretrained_vae_name_or_path PRETRAINED_VAE_NAME_OR_PATH]\n",
            "                           [--revision REVISION]\n",
            "                           [--tokenizer_name TOKENIZER_NAME]\n",
            "                           [--instance_data_dir INSTANCE_DATA_DIR]\n",
            "                           [--class_data_dir CLASS_DATA_DIR]\n",
            "                           [--instance_prompt INSTANCE_PROMPT]\n",
            "                           [--class_prompt CLASS_PROMPT]\n",
            "                           [--save_sample_prompt SAVE_SAMPLE_PROMPT]\n",
            "                           [--save_sample_negative_prompt SAVE_SAMPLE_NEGATIVE_PROMPT]\n",
            "                           [--n_save_sample N_SAVE_SAMPLE]\n",
            "                           [--save_guidance_scale SAVE_GUIDANCE_SCALE]\n",
            "                           [--save_infer_steps SAVE_INFER_STEPS]\n",
            "                           [--pad_tokens] [--with_prior_preservation]\n",
            "                           [--prior_loss_weight PRIOR_LOSS_WEIGHT]\n",
            "                           [--num_class_images NUM_CLASS_IMAGES]\n",
            "                           [--output_dir OUTPUT_DIR] [--seed SEED]\n",
            "                           [--resolution RESOLUTION] [--center_crop]\n",
            "                           [--train_text_encoder]\n",
            "                           [--train_batch_size TRAIN_BATCH_SIZE]\n",
            "                           [--sample_batch_size SAMPLE_BATCH_SIZE]\n",
            "                           [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
            "                           [--max_train_steps MAX_TRAIN_STEPS]\n",
            "                           [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
            "                           [--gradient_checkpointing]\n",
            "                           [--learning_rate LEARNING_RATE] [--scale_lr]\n",
            "                           [--lr_scheduler LR_SCHEDULER]\n",
            "                           [--lr_warmup_steps LR_WARMUP_STEPS]\n",
            "                           [--use_8bit_adam] [--adam_beta1 ADAM_BETA1]\n",
            "                           [--adam_beta2 ADAM_BETA2]\n",
            "                           [--adam_weight_decay ADAM_WEIGHT_DECAY]\n",
            "                           [--adam_epsilon ADAM_EPSILON]\n",
            "                           [--max_grad_norm MAX_GRAD_NORM] [--push_to_hub]\n",
            "                           [--hub_token HUB_TOKEN]\n",
            "                           [--hub_model_id HUB_MODEL_ID]\n",
            "                           [--logging_dir LOGGING_DIR]\n",
            "                           [--log_interval LOG_INTERVAL]\n",
            "                           [--save_interval SAVE_INTERVAL]\n",
            "                           [--save_min_steps SAVE_MIN_STEPS]\n",
            "                           [--mixed_precision {no,fp16,bf16}]\n",
            "                           [--not_cache_latents] [--hflip]\n",
            "                           [--local_rank LOCAL_RANK]\n",
            "                           [--concepts_list CONCEPTS_LIST]\n",
            "                           [--read_prompts_from_txts]\n",
            "train_dreambooth.py: error: the following arguments are required: --pretrained_model_name_or_path\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "019c4715",
        "outputId": "79e2bd5e-0281-4087-bc3f-ba63169b4a91"
      },
      "source": [
        "! python3 train_dreambooth.py \\\n",
        "  --pretrained_model_name_or_path=\"runwayml/stable-diffusion-v1-5\"\\\n",
        "  --output_dir=$output_dir \\\n",
        "  --concepts_list=\"concepts_list.json\" \\\n",
        "  --with_prior_preservation --prior_loss_weight=1.0 \\\n",
        "  --instance_data_dir=\"/content/dataset_dave\" \\\n",
        "  --class_data_dir=\"/content/dataset_dave\" \\\n",
        "  --instance_prompt=\"photo of zwx person\" \\\n",
        "  --class_prompt=\"photo of a person\" \\\n",
        "  --num_class_images=$num_class_images \\\n",
        "  --resolution=512 \\\n",
        "  --train_batch_size=1 \\\n",
        "  --sample_batch_size=1 \\\n",
        "  --gradient_accumulation_steps=1 \\\n",
        "  --learning_rate=$learning_rate \\\n",
        "  --lr_scheduler=\"constant\" \\\n",
        "  --lr_warmup_steps=$lr_warmup_steps \\\n",
        "  --max_train_steps=$max_num_steps \\\n",
        "  --mixed_precision=\"fp16\" \\\n",
        "  --gradient_checkpointing \\\n",
        "  --use_8bit_adam"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-09-18 07:04:14.372762: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1758179054.393073    7148 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1758179054.399340    7148 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1758179054.414726    7148 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758179054.414762    7148 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758179054.414765    7148 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758179054.414767    7148 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-09-18 07:04:14.419370: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "config.json: 100% 547/547 [00:00<00:00, 3.79MB/s]\n",
            "vae/diffusion_pytorch_model.safetensors: 100% 335M/335M [00:01<00:00, 275MB/s]\n",
            "model_index.json: 100% 541/541 [00:00<00:00, 6.25MB/s]\n",
            "Fetching 11 files:   0% 0/11 [00:00<?, ?it/s]\n",
            "special_tokens_map.json:   0% 0.00/472 [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "special_tokens_map.json: 100% 472/472 [00:00<00:00, 521kB/s]\n",
            "\n",
            "scheduler_config.json:   0% 0.00/308 [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "\n",
            "scheduler_config.json: 100% 308/308 [00:00<00:00, 392kB/s]\n",
            "\n",
            "config.json: 100% 617/617 [00:00<00:00, 5.23MB/s]\n",
            "\n",
            "preprocessor_config.json: 100% 342/342 [00:00<00:00, 4.75MB/s]\n",
            "merges.txt: 525kB [00:00, 24.7MB/s]\n",
            "\n",
            "tokenizer_config.json: 100% 806/806 [00:00<00:00, 8.07MB/s]\n",
            "vocab.json: 1.06MB [00:00, 42.5MB/s]\n",
            "\n",
            "text_encoder/model.safetensors:   0% 0.00/492M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "config.json: 100% 743/743 [00:00<00:00, 8.75MB/s]\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:   0% 0.00/3.44G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "text_encoder/model.safetensors:   0% 21.5k/492M [00:00<3:24:20, 40.1kB/s]\u001b[A\n",
            "text_encoder/model.safetensors:   2% 10.5M/492M [00:00<00:24, 19.5MB/s]  \u001b[A\n",
            "text_encoder/model.safetensors:  12% 58.5M/492M [00:00<00:03, 109MB/s] \u001b[A\n",
            "text_encoder/model.safetensors:  35% 174M/492M [00:00<00:00, 336MB/s] \u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:   0% 34.1k/3.44G [00:00<27:01:26, 35.3kB/s]\u001b[A\u001b[A\n",
            "text_encoder/model.safetensors:  51% 253M/492M [00:01<00:00, 271MB/s]\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:   0% 1.44M/3.44G [00:01<41:08, 1.39MB/s]   \u001b[A\u001b[A\n",
            "text_encoder/model.safetensors:  62% 305M/492M [00:03<00:02, 64.6MB/s]\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:   1% 18.8M/3.44G [00:03<08:45, 6.50MB/s]\u001b[A\u001b[A\n",
            "text_encoder/model.safetensors:  86% 425M/492M [00:03<00:00, 117MB/s] \u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:   4% 153M/3.44G [00:03<00:50, 65.7MB/s] \u001b[A\u001b[A\n",
            "text_encoder/model.safetensors: 100% 492M/492M [00:03<00:00, 125MB/s]\n",
            "Fetching 11 files:  45% 5/11 [00:04<00:05,  1.12it/s]\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:   6% 220M/3.44G [00:04<00:38, 84.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:   8% 287M/3.44G [00:04<00:26, 117MB/s] \u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  10% 354M/3.44G [00:04<00:19, 160MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  14% 488M/3.44G [00:07<00:42, 69.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  18% 622M/3.44G [00:07<00:24, 113MB/s] \u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  22% 757M/3.44G [00:07<00:15, 170MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  26% 891M/3.44G [00:08<00:11, 215MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  28% 958M/3.44G [00:08<00:12, 198MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  30% 1.02G/3.44G [00:08<00:12, 197MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  32% 1.09G/3.44G [00:09<00:12, 191MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  34% 1.16G/3.44G [00:11<00:30, 75.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  40% 1.36G/3.44G [00:11<00:13, 151MB/s] \u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  45% 1.56G/3.44G [00:12<00:07, 235MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  49% 1.70G/3.44G [00:12<00:07, 240MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  51% 1.76G/3.44G [00:12<00:07, 224MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  53% 1.83G/3.44G [00:13<00:06, 234MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  55% 1.90G/3.44G [00:13<00:07, 213MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  57% 1.96G/3.44G [00:15<00:16, 87.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  63% 2.17G/3.44G [00:15<00:07, 166MB/s] \u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  67% 2.30G/3.44G [00:16<00:05, 208MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  69% 2.37G/3.44G [00:16<00:04, 222MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  71% 2.43G/3.44G [00:16<00:03, 258MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  77% 2.63G/3.44G [00:17<00:02, 307MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  79% 2.70G/3.44G [00:19<00:07, 97.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  84% 2.90G/3.44G [00:19<00:03, 168MB/s] \u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  90% 3.10G/3.44G [00:20<00:01, 257MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  96% 3.30G/3.44G [00:20<00:00, 328MB/s]\u001b[A\u001b[A\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors: 100% 3.44G/3.44G [00:23<00:00, 146MB/s]\n",
            "Fetching 11 files: 100% 11/11 [00:24<00:00,  2.20s/it]\n",
            "Loading pipeline components...:  33% 2/6 [00:00<00:01,  3.21it/s]`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading pipeline components...: 100% 6/6 [00:00<00:00,  6.82it/s]\n",
            "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
            "Generating class images: 100% 110/110 [06:14<00:00,  3.40s/it]\n",
            "/usr/local/lib/python3.12/dist-packages/diffusers/configuration_utils.py:250: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a scheduler, please use <class 'diffusers.schedulers.scheduling_ddpm.DDPMScheduler'>.from_pretrained(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.\n",
            "  deprecate(\"config-passed-as-path\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
            "Caching latents: 100% 120/120 [00:23<00:00,  5.01it/s]\n",
            "Steps: 100% 800/800 [12:34<00:00,  1.13it/s, loss=0.276, lr=1e-6]\n",
            "Loading pipeline components...: 100% 6/6 [00:00<00:00, 88.34it/s]\n",
            "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
            "[*] Weights saved at /content/stable_diffusion_weights/zwx/800\n",
            "Steps: 100% 800/800 [12:50<00:00,  1.04it/s, loss=0.276, lr=1e-6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sure! Let’s go **step by step** through this output so you understand what’s happening during your DreamBooth training run. I’ll break it into sections and explain what each part means, why it happens, and whether it’s a problem.\n",
        "\n",
        "---\n",
        "\n",
        "## **1️⃣ CUDA / TensorFlow warnings**\n",
        "\n",
        "```\n",
        "2025-09-18 07:04:14.372762: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
        "...\n",
        "W0000 00:00:1758179054.414767    7148 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
        "2025-09-18 07:04:14.419370: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions...\n",
        "```\n",
        "\n",
        "**What it is:**\n",
        "\n",
        "* TensorFlow is trying to initialize CUDA libraries: **cuFFT, cuDNN, cuBLAS**.\n",
        "* It detects that these libraries are already loaded (probably by PyTorch or XLA).\n",
        "\n",
        "**Impact:**\n",
        "\n",
        "* These are **warnings, not errors**.\n",
        "* Training continues normally in PyTorch (your DreamBooth uses PyTorch, not TensorFlow).\n",
        "* You don’t need to worry unless you actually see CUDA errors stopping training.\n",
        "\n",
        "---\n",
        "\n",
        "## **2️⃣ Fetching model files**\n",
        "\n",
        "```\n",
        "config.json: 100% 547/547 [00:00<00:00, 3.79MB/s]\n",
        "vae/diffusion_pytorch_model.safetensors: 100% 335M/335M [00:01<00:00, 275MB/s]\n",
        "...\n",
        "text_encoder/model.safetensors: 100% 492M/492M [00:03<00:00, 125MB/s]\n",
        "unet/diffusion_pytorch_model.safetensors: 100% 3.44G/3.44G [00:23<00:00, 146MB/s]\n",
        "```\n",
        "\n",
        "**What it is:**\n",
        "\n",
        "* DreamBooth downloads the **pretrained Stable Diffusion components** from Hugging Face:\n",
        "\n",
        "  1. **VAE** (`vae/diffusion_pytorch_model.safetensors`) – encoder/decoder for images.\n",
        "  2. **UNet** (`unet/diffusion_pytorch_model.safetensors`) – the main generative model.\n",
        "  3. **Text encoder** (`text_encoder/model.safetensors`) – converts prompts into embeddings.\n",
        "  4. Configs, tokenizer, scheduler, etc.\n",
        "\n",
        "**Why important:**\n",
        "\n",
        "* These are the **base models** DreamBooth fine-tunes on your images.\n",
        "* Without them, the training cannot start.\n",
        "\n",
        "**Observation:**\n",
        "\n",
        "* You can see the download progress for large files (`3.44G` UNet) and smaller files.\n",
        "* Slow speeds at first are normal if the internet is slow; caching helps for next runs.\n",
        "\n",
        "---\n",
        "\n",
        "## **3️⃣ Safety checker warning**\n",
        "\n",
        "```\n",
        "You have disabled the safety checker for ... by passing `safety_checker=None`...\n",
        "```\n",
        "\n",
        "**What it is:**\n",
        "\n",
        "* Diffusers has a **safety filter** to block NSFW content.\n",
        "* You disabled it (`safety_checker=None`) to avoid warnings during training or generation.\n",
        "\n",
        "**Impact:**\n",
        "\n",
        "* No effect on DreamBooth training itself.\n",
        "* Only matters if you plan to deploy the model publicly; make sure you comply with license rules.\n",
        "\n",
        "---\n",
        "\n",
        "## **4️⃣ Generating class images**\n",
        "\n",
        "```\n",
        "Generating class images: 100% 110/110 [06:14<00:00, 3.40s/it]\n",
        "```\n",
        "\n",
        "**What it is:**\n",
        "\n",
        "* DreamBooth generates **class images** (generic “person”) to prevent overfitting.\n",
        "* `--with_prior_preservation` + `--prior_loss_weight` uses these images to regularize training.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "* You have 10 instance images of “zwx”.\n",
        "* DreamBooth generates 110 generic person images (class images) to help the model **not forget general human features**.\n",
        "\n",
        "**Impact if skipped:**\n",
        "\n",
        "* Model may **overfit** to the small instance dataset → outputs will only look like your few images, losing generalization.\n",
        "\n",
        "---\n",
        "\n",
        "## **5️⃣ Caching latents**\n",
        "\n",
        "```\n",
        "Caching latents: 100% 120/120 [00:23<00:00, 5.01it/s]\n",
        "```\n",
        "\n",
        "**What it is:**\n",
        "\n",
        "* Converts images into **latent space** representations before training.\n",
        "* Latents are smaller tensors that UNet operates on instead of full images.\n",
        "\n",
        "**Why important:**\n",
        "\n",
        "* Speeds up training: model doesn’t recompute encoder every step.\n",
        "* Saves VRAM and computation.\n",
        "\n",
        "---\n",
        "\n",
        "## **6️⃣ Training steps**\n",
        "\n",
        "```\n",
        "Steps: 100% 800/800 [12:34<00:00, 1.13it/s, loss=0.276, lr=1e-6]\n",
        "```\n",
        "\n",
        "**What it is:**\n",
        "\n",
        "* Model has finished **800 training steps**.\n",
        "* `loss=0.276` → shows how well the model is fitting the instance+class images.\n",
        "* `lr=1e-6` → current learning rate (your constant scheduler).\n",
        "\n",
        "**Key notes:**\n",
        "\n",
        "* Loss decreasing → model is learning.\n",
        "* Training time: 12 minutes 34 seconds → reasonable for a small DreamBooth setup.\n",
        "\n",
        "---\n",
        "\n",
        "## **7️⃣ Pipeline loaded**\n",
        "\n",
        "```\n",
        "Loading pipeline components...: 100% 6/6 [00:00<00:00, 88.34it/s]\n",
        "```\n",
        "\n",
        "**What it is:**\n",
        "\n",
        "* After training, the script reloads the **Stable Diffusion pipeline** with your fine-tuned weights.\n",
        "* Ready for generating images using `instance_prompt` tokens like “zwx”.\n",
        "\n",
        "---\n",
        "\n",
        "## **8️⃣ Weights saved**\n",
        "\n",
        "```\n",
        "[*] Weights saved at /content/stable_diffusion_weights/zwx/800\n",
        "```\n",
        "\n",
        "**What it is:**\n",
        "\n",
        "* Trained model weights are saved to disk.\n",
        "* `800` → corresponds to number of training steps.\n",
        "* You can now load this model for inference using your token “zwx”.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "```python\n",
        "from diffusers import StableDiffusionPipeline\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\"/content/stable_diffusion_weights/zwx/800\")\n",
        "image = pipe(\"photo of zwx person wearing sunglasses\").images[0]\n",
        "image.save(\"zwx_sunglasses.png\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ **Summary of the log**\n",
        "\n",
        "| Stage                  | What happened                               | Importance / Effect                        |\n",
        "| ---------------------- | ------------------------------------------- | ------------------------------------------ |\n",
        "| CUDA / TF warnings     | Libraries already loaded                    | Harmless, training continues               |\n",
        "| Model files fetched    | VAE, UNet, text encoder, configs            | Needed for fine-tuning                     |\n",
        "| Safety checker         | Disabled                                    | No effect on training, only for deployment |\n",
        "| Class images generated | 110 generic images                          | Prevents overfitting (prior preservation)  |\n",
        "| Latents cached         | 120 latents                                 | Speeds up training                         |\n",
        "| Training               | 800 steps, loss 0.276                       | Model learned the subject                  |\n",
        "| Pipeline loaded        | Ready for inference                         | Can generate images with new token         |\n",
        "| Weights saved          | `/content/stable_diffusion_weights/zwx/800` | Fine-tuned model stored for future use     |\n",
        "\n",
        "---\n",
        "\n",
        "If you want, I can **draw a full visual flow** showing how **instance images, class images, latents, UNet, text encoder, and gradient updates** all work together during DreamBooth training — it makes this entire log much easier to understand.\n",
        "\n",
        "Do you want me to make that diagram?\n"
      ],
      "metadata": {
        "id": "9EjiSls4uBXh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nice — I’ll walk through every flag you used, what it does, why it matters, recommended values, and any gotchas / interactions to watch for. I’ll also give a copy-paste-ready example (with `concepts_list.json` sample) at the end so you can run it safely.\n",
        "\n",
        "# What each argument means (line-by-line)\n",
        "\n",
        "`--pretrained_model_name_or_path=\"runwayml/stable-diffusion-v1-5\"`\n",
        "\n",
        "* **What:** The base model you fine-tune from. Can be a Hugging Face model id (hub) or a local folder with a pretrained Stable Diffusion model.\n",
        "* **Why it matters:** All weights (UNet, VAE, text encoder) are loaded from here. If it’s a gated model you must be authenticated (`huggingface-cli login`) or pass a token.\n",
        "* **Tip:** Use `runwayml/stable-diffusion-v1-5` (or a compatible SD checkpoint). If you use a local path, set it to the folder containing model subfolders (unet, vae, text\\_encoder, etc.).\n",
        "\n",
        "`--output_dir=$output_dir`\n",
        "\n",
        "* **What:** Where trained checkpoints / final pipeline are saved.\n",
        "* **Why:** The script writes `output_dir/<step>/` (and final save at end). If you want intermediate checkpoints, tune `--save_interval`.\n",
        "* **Tip:** Use a path on disk with enough free space.\n",
        "\n",
        "`--concepts_list=\"concepts_list.json\"`\n",
        "\n",
        "* **What:** JSON file describing one or more concepts to train. If present, the script loads this list and **ignores** the single `--instance_prompt`/`--instance_data_dir` flags.\n",
        "* **Format:** list of objects with keys: `instance_prompt`, `class_prompt`, `instance_data_dir`, `class_data_dir`. (Example below.)\n",
        "* **When to use:** Use when you want to train multiple concepts in one run or supply differing prompts/dirs per concept.\n",
        "\n",
        "`--with_prior_preservation --prior_loss_weight=1.0`\n",
        "\n",
        "* **What:** Enables DreamBooth’s *prior preservation* technique. It adds an extra loss term that keeps the model from “collapsing” widely to reproduce only your few instance images. `prior_loss_weight` scales that extra loss.\n",
        "* **Why:** Prevents overfitting and keeps the model’s notion of the general class (e.g., “a person”) while learning the specific subject.\n",
        "* **Tip:** `prior_loss_weight=1.0` is a common default. If you see over-regularization (subject not learned) lower it; if you see overfitting/inversion artifacts increase it.\n",
        "\n",
        "`--instance_data_dir=\"/content/dataset_dave\"`\n",
        "\n",
        "* **What:** Folder with your instance (subject) images to teach the model (the photos of the person/object you want to learn).\n",
        "* **Tip:** 5–20 high-quality images of multiple poses/backgrounds are commonly used. More images let you use fewer steps.\n",
        "\n",
        "`--class_data_dir=\"/content/dataset_dave\"`\n",
        "\n",
        "* **What:** Folder with class images used for prior preservation (generic images for that class, e.g., many photos of other people).\n",
        "* **Important:** **Do not** use the same set for `instance_data_dir` and `class_data_dir`. If they’re identical, prior preservation will do nothing useful — you should provide *diverse* class images (many different people) so the model learns “person” while preserving uniqueness of your instance. The script *can* generate extra class images if not enough exist (it will sample from the pretrained SD pipeline).\n",
        "\n",
        "`--instance_prompt=\"photo of zwx person\"`\n",
        "\n",
        "* **What:** The prompt string used to describe your instance images during training. This should include a unique identifier token (e.g., an unusual word like `zwx`, `sks`, or a made-up token like `<yourtoken>`).\n",
        "* **Why:** The unique token ties the subject to the concept in the model’s latent space so the model can later generate images of that subject by using this token in a prompt.\n",
        "* **Tip:** Use something unlikely to collide with normal text (e.g., `photo of zwx person` or `a photo of <myname> person`). If you later want to generate `photo of zwx person in a hat`, the model will associate `zwx` with the learned subject.\n",
        "\n",
        "`--class_prompt=\"photo of a person\"`\n",
        "\n",
        "* **What:** Generic prompt used for class images. It should describe the general class, e.g., “photo of a person”.\n",
        "* **Why:** Paired with prior preservation to keep model’s general knowledge of that class.\n",
        "\n",
        "`--num_class_images=$num_class_images`\n",
        "\n",
        "* **What:** Target number of class images used for prior preservation. The script will sample/generate additional class images to reach this number if needed.\n",
        "* **Tip:** Typical values: 100–200+. The higher the better for strong prior preservation, but generates more class images and uses more disk/time.\n",
        "\n",
        "`--resolution=512`\n",
        "\n",
        "* **What:** Training image size (images are resized/cropped to this). 512 is standard for many SD models.\n",
        "* **Why:** The model architecture expects a multiple of 64 typically; using the same resolution as the pretrained model avoids mismatch.\n",
        "* **Tip:** Use the model’s native resolution (512 or 768 depending on model). Larger resolution = more VRAM and slower training.\n",
        "\n",
        "`--train_batch_size=1`\n",
        "\n",
        "* **What:** Per-device batch size for training.\n",
        "* **Why:** Larger batch sizes speed training but need more GPU memory. DreamBooth commonly uses `1`. You can use gradient accumulation to simulate larger effective batch sizes.\n",
        "\n",
        "`--sample_batch_size=1`\n",
        "\n",
        "* **What:** Batch size used when generating class images (sampling pipeline). Keep small to avoid GPU OOM during generation.\n",
        "\n",
        "`--gradient_accumulation_steps=1`\n",
        "\n",
        "* **What:** Number of mini-batches to accumulate gradients over before stepping optimizer.\n",
        "* **Why:** Useful to simulate larger batch sizes if GPU memory is limited: `effective_batch = train_batch_size * gradient_accumulation_steps * num_gpus`.\n",
        "* **Tip:** If you want an effective batch of 4 but only fit batch size 1, set `gradient_accumulation_steps=4`.\n",
        "\n",
        "`--learning_rate=$learning_rate`\n",
        "\n",
        "* **What:** Base learning rate for optimizer. DreamBooth often uses small LRs: `1e-6` to `5e-6` (common starting point `5e-6`).\n",
        "* **Tip:** Too large → destroys pretrained weights; too small → slow/ineffective learning. Tune carefully.\n",
        "\n",
        "`--lr_scheduler=\"constant\"`\n",
        "\n",
        "* **What:** Which LR scheduler to use. `\"constant\"` keeps LR fixed (but may support warmup if implemented). Other options: `linear`, `cosine`, `polynomial`, etc.\n",
        "* **Tip:** Constant or constant\\_with\\_warmup is common for finetuning.\n",
        "\n",
        "`--lr_warmup_steps=$lr_warmup_steps`\n",
        "\n",
        "* **What:** Number of warmup steps at start where LR ramps up from 0 to `learning_rate`.\n",
        "* **Why:** Small warmup stabilizes training at start. Typical `lr_warmup_steps` is small (0–100).\n",
        "\n",
        "`--max_train_steps=$max_num_steps`\n",
        "\n",
        "* **What:** Total optimizer steps to perform. Overrides `num_train_epochs` if set.\n",
        "* **Guideline:** DreamBooth common ranges:\n",
        "\n",
        "  * If you have 10–20 images: **800–2000** steps often reasonable.\n",
        "  * If you have many images, scale steps accordingly.\n",
        "* **Tip:** Monitor outputs; you can stop early if results are good, or increase if underfitting.\n",
        "\n",
        "`--mixed_precision=\"fp16\"`\n",
        "\n",
        "* **What:** Use half-precision (FP16) training for memory and speed. Alternatives: `bf16` (if hardware supports) or `no`.\n",
        "* **Why:** FP16 reduces VRAM usage and speeds up many GPUs.\n",
        "* **Caveats:** BF16 is safer numerically but requires GPU and PyTorch support. FP16 can cause instabilities on some setups — use gradient scaling/AMP (handled by `accelerate`). If you see NaNs, try `no` or `bf16` (if supported).\n",
        "\n",
        "`--gradient_checkpointing`\n",
        "\n",
        "* **What:** Activates gradient checkpointing to reduce memory use by recomputing activations during backward pass.\n",
        "* **Tradeoff:** Uses less memory but increases compute time (\\~10–30% slower).\n",
        "\n",
        "`--use_8bit_adam`\n",
        "\n",
        "* **What:** Use 8-bit Adam optimizer (from `bitsandbytes`) which keeps optimizer state in 8-bit to drastically reduce memory used by optimizer states.\n",
        "* **Requires:** `pip install bitsandbytes` and proper CUDA / driver compatibility.\n",
        "* **Tip:** Combine `use_8bit_adam` + `gradient_checkpointing` + `fp16` to fit DreamBooth on 12–16GB GPUs.\n",
        "\n",
        "---\n",
        "\n",
        "# Extra script behavior & important interactions\n",
        "\n",
        "* **`concepts_list.json` vs CLI single flags:** If `--concepts_list` is provided, the script **loads** that and ignores the single `--instance_prompt`/`--instance_data_dir` you passed. Use one method, not both, unless your JSON intentionally depends on CLI variables.\n",
        "* **Caching latents:** By default the script caches latents (`vae.encode(...)`) to speed training and save VAE memory later. If you set `--not_cache_latents`, training will compute latents on the fly (less disk & memory to store caches, but slower).\n",
        "* **Saving:** The default `save_interval` in the script is large (10,000). If your `max_train_steps` is smaller, you’ll still get a final save at the end (the script calls `save_weights` at completion). If you want intermediate checkpoints, set `--save_interval` smaller.\n",
        "* **Class images generation:** If `class_data_dir` doesn’t have enough images, the script uses the StableDiffusion pipeline to generate more class images (using `sample_batch_size` and `save_infer_steps`). This requires the pipeline to be loaded (and that uses VRAM).\n",
        "* **Unique identifier token in `instance_prompt`:** For classic DreamBooth you usually include an identifier token (e.g., `a photo of <VAN_GOGH_STYLE> dog`), but the script as written doesn’t automatically add new tokens to the tokenizer / text encoder vocabulary — so use an uncommon short token (like `zwx` or `sks`) to avoid collisions. If you want to actually add a new token embedding, more advanced token insertion/training is required (not in this script).\n",
        "* **If you use gated models (like official Stable Diffusion weights):** you must be authenticated to Hugging Face (`huggingface-cli login`) or pass `--hub_token`/environment var.\n",
        "\n",
        "---\n",
        "\n",
        "# Example `concepts_list.json`\n",
        "\n",
        "If you want to train multiple concepts or prefer JSON config, here’s an example file:\n",
        "\n",
        "```json\n",
        "[\n",
        "  {\n",
        "    \"instance_prompt\": \"photo of zwx person\",\n",
        "    \"class_prompt\": \"photo of a person\",\n",
        "    \"instance_data_dir\": \"/content/dataset_dave\",\n",
        "    \"class_data_dir\": \"/content/class_images_person\"\n",
        "  }\n",
        "]\n",
        "```\n",
        "\n",
        "Place that as `concepts_list.json` and then call the script with `--concepts_list=\"concepts_list.json\"`. The script will iterate concepts.\n",
        "\n",
        "---\n",
        "\n",
        "# Recommended starting hyperparameters (common DreamBooth recipe)\n",
        "\n",
        "* `train_batch_size=1`\n",
        "* `gradient_accumulation_steps=1` (increase if you want larger effective batch)\n",
        "* `learning_rate=5e-6` (try 1e-6 → 5e-6)\n",
        "* `max_train_steps=800` → increase to 1000–2000 for better fidelity with very few images\n",
        "* `num_class_images=100` (or 200 if you want stronger prior)\n",
        "* `mixed_precision=fp16` (if you have an NVIDIA GPU that supports it)\n",
        "* `gradient_checkpointing` = ON (saves VRAM)\n",
        "* `use_8bit_adam` = ON if you installed `bitsandbytes` and want to save optimizer memory\n",
        "\n",
        "---\n",
        "\n",
        "# Example full command (copy-paste, adjust variables)\n",
        "\n",
        "```bash\n",
        "export output_dir=\"./dreambooth_dave\"\n",
        "export num_class_images=200\n",
        "export learning_rate=5e-6\n",
        "export lr_warmup_steps=0\n",
        "export max_num_steps=1000\n",
        "export model_sd=\"runwayml/stable-diffusion-v1-5\"\n",
        "\n",
        "python3 train_dreambooth.py \\\n",
        "  --pretrained_model_name_or_path=\"$model_sd\" \\\n",
        "  --output_dir=\"$output_dir\" \\\n",
        "  --concepts_list=\"concepts_list.json\" \\\n",
        "  --with_prior_preservation --prior_loss_weight=1.0 \\\n",
        "  --instance_data_dir=\"/content/dataset_dave\" \\\n",
        "  --class_data_dir=\"/content/class_images_person\" \\\n",
        "  --instance_prompt=\"photo of zwx person\" \\\n",
        "  --class_prompt=\"photo of a person\" \\\n",
        "  --num_class_images=$num_class_images \\\n",
        "  --resolution=512 \\\n",
        "  --train_batch_size=1 \\\n",
        "  --sample_batch_size=1 \\\n",
        "  --gradient_accumulation_steps=1 \\\n",
        "  --learning_rate=$learning_rate \\\n",
        "  --lr_scheduler=\"constant\" \\\n",
        "  --lr_warmup_steps=$lr_warmup_steps \\\n",
        "  --max_train_steps=$max_num_steps \\\n",
        "  --mixed_precision=\"fp16\" \\\n",
        "  --gradient_checkpointing \\\n",
        "  --use_8bit_adam\n",
        "```\n",
        "\n",
        "**Note:** make sure you are logged in to Hugging Face if the model is gated:\n",
        "\n",
        "```bash\n",
        "huggingface-cli login\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# Final checklist before you run\n",
        "\n",
        "* `huggingface-cli login` (if model gated)\n",
        "* `concepts_list.json` exists and points to the correct folders, or remove `--concepts_list` and pass `--instance_data_dir`/`--instance_prompt` directly.\n",
        "* `class_data_dir` contains diverse class images (not identical to instance images).\n",
        "* `bitsandbytes` installed if using `--use_8bit_adam`.\n",
        "* GPU memory: if you run out of memory, enable `gradient_checkpointing`, `use_8bit_adam`, reduce `resolution`, or reduce `train_batch_size` / use `gradient_accumulation_steps`.\n",
        "\n",
        "---\n",
        "\n",
        "If you want, I can:\n",
        "\n",
        "* Generate a ready `concepts_list.json` from your dataset paths, or\n",
        "* Suggest specific `max_train_steps`/`learning_rate` tuned to the number of instance images you have (tell me how many images you have).\n"
      ],
      "metadata": {
        "id": "WUDjy5T3rlOk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Absolutely! Let’s break each of these options **thoroughly**, with examples, why they matter, and what happens if you skip them. I’ll also explain the **practical impact** on training DreamBooth models.\n",
        "\n",
        "---\n",
        "\n",
        "## **1️⃣ `--concepts_list=\"concepts_list.json\"`**\n",
        "\n",
        "**What it does:**\n",
        "\n",
        "* Allows you to train **one or more concepts** (subjects) in a single run.\n",
        "* Each concept can have its own images and prompts.\n",
        "* If this is provided, the script **ignores** single flags like `--instance_prompt` and `--instance_data_dir`.\n",
        "\n",
        "**Format of the JSON:**\n",
        "\n",
        "```json\n",
        "[\n",
        "  {\n",
        "    \"instance_prompt\": \"photo of zwx person\",\n",
        "    \"class_prompt\": \"photo of a person\",\n",
        "    \"instance_data_dir\": \"/content/dataset_dave\",\n",
        "    \"class_data_dir\": \"/content/class_images_person\"\n",
        "  },\n",
        "  {\n",
        "    \"instance_prompt\": \"photo of catxyz\",\n",
        "    \"class_prompt\": \"photo of a cat\",\n",
        "    \"instance_data_dir\": \"/content/dataset_cat\",\n",
        "    \"class_data_dir\": \"/content/class_images_cat\"\n",
        "  }\n",
        "]\n",
        "```\n",
        "\n",
        "**Example:**\n",
        "\n",
        "* You want to teach the model **two subjects**: a person (`zwx`) and a cat (`catxyz`).\n",
        "* You create the above JSON. Now one command trains both concepts in one run.\n",
        "\n",
        "**Why important:**\n",
        "\n",
        "* Simplifies multi-concept training.\n",
        "* Ensures proper separation of prompts, datasets, and prior images per concept.\n",
        "\n",
        "**If missing:**\n",
        "\n",
        "* You can still train one concept using `--instance_prompt` and `--instance_data_dir`, but **multi-concept training** would require multiple runs manually.\n",
        "\n",
        "---\n",
        "\n",
        "## **2️⃣ `--instance_prompt=\"photo of zwx person\"`**\n",
        "\n",
        "**What it does:**\n",
        "\n",
        "* This is the **text prompt** describing your instance images during training.\n",
        "* The model associates the unique token `zwx` with your subject.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "* You have 10 images of a person named Dave.\n",
        "* You set `--instance_prompt=\"photo of zwx person\"`.\n",
        "* After training, you can generate images like:\n",
        "\n",
        "  ```text\n",
        "  \"photo of zwx person wearing a hat\"\n",
        "  ```\n",
        "\n",
        "  and the model knows “zwx” = Dave.\n",
        "\n",
        "**Why important:**\n",
        "\n",
        "* Without this, the model cannot know which part of the image is your subject.\n",
        "* This unique token allows the model to differentiate your instance from other generic subjects.\n",
        "\n",
        "**If missing:**\n",
        "\n",
        "* The model may just learn generic “person” features, losing specificity.\n",
        "* Outputs won’t look like the person/object you want.\n",
        "\n",
        "---\n",
        "\n",
        "## **3️⃣ `--gradient_accumulation_steps=1`**\n",
        "\n",
        "**What it does:**\n",
        "\n",
        "* Accumulates gradients over multiple mini-batches **before updating the model weights**.\n",
        "* Simulates a **larger batch size** without needing more GPU memory.\n",
        "\n",
        "**Formula:**\n",
        "\n",
        "```\n",
        "effective_batch_size = train_batch_size * gradient_accumulation_steps * num_gpus\n",
        "```\n",
        "\n",
        "**Example:**\n",
        "\n",
        "* `train_batch_size=1`\n",
        "* `gradient_accumulation_steps=4`\n",
        "* Effective batch size = 4\n",
        "* The optimizer updates only after 4 images have been processed.\n",
        "\n",
        "**Why important:**\n",
        "\n",
        "* Helps train on GPUs with **limited memory** while maintaining stable gradient updates.\n",
        "* Larger batch sizes = smoother training, fewer oscillations.\n",
        "\n",
        "**If missing:**\n",
        "\n",
        "* Batch size = 1 in the above example.\n",
        "* Could cause unstable training if your dataset is tiny or the learning rate is too high.\n",
        "\n",
        "---\n",
        "\n",
        "## **4️⃣ `--lr_scheduler=\"constant\"`**\n",
        "\n",
        "**What it does:**\n",
        "\n",
        "* Controls how learning rate (LR) changes during training.\n",
        "* `\"constant\"` keeps the learning rate **the same** for all steps (may still support warmup).\n",
        "\n",
        "**Other options:**\n",
        "\n",
        "* `\"linear\"`: LR decreases linearly to 0.\n",
        "* `\"cosine\"`: LR follows a cosine decay curve.\n",
        "* `\"polynomial\"`: LR decreases polynomially.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "* `learning_rate=5e-6`\n",
        "* `lr_scheduler=constant` → keeps 5e-6 throughout training.\n",
        "* Simple, stable for DreamBooth fine-tuning.\n",
        "\n",
        "**Why important:**\n",
        "\n",
        "* LR schedule helps the model converge better.\n",
        "* Constant LR is good for small fine-tuning (few images).\n",
        "\n",
        "**If missing:**\n",
        "\n",
        "* Default scheduler may be used (depends on script).\n",
        "* Could cause slow convergence or overfitting if LR not suited.\n",
        "\n",
        "---\n",
        "\n",
        "## **5️⃣ `--mixed_precision=\"fp16\"`**\n",
        "\n",
        "**What it does:**\n",
        "\n",
        "* Uses **half-precision floats (16-bit)** instead of full 32-bit floats for training.\n",
        "* Saves GPU memory and speeds up training.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "* Standard FP32 might use **16 GB VRAM** for DreamBooth.\n",
        "* FP16 reduces usage to **\\~8–10 GB VRAM** on the same model.\n",
        "\n",
        "**Why important:**\n",
        "\n",
        "* Allows training large models (SD) on GPUs with limited VRAM.\n",
        "* Speeds up computations because FP16 math is faster on modern GPUs (Tensor Cores).\n",
        "\n",
        "**If missing:**\n",
        "\n",
        "* Training will use FP32 → higher memory usage.\n",
        "* Might not fit in your GPU if VRAM is small.\n",
        "\n",
        "**Caveats:**\n",
        "\n",
        "* FP16 can cause **NaN / instability** if gradients explode.\n",
        "* Alternative: `bf16` (if GPU supports) → safer numerically.\n",
        "\n",
        "---\n",
        "\n",
        "## **6️⃣ `--gradient_checkpointing`**\n",
        "\n",
        "**What it does:**\n",
        "\n",
        "* Reduces GPU memory usage by **recomputing activations during backward pass** instead of storing them.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "* Without checkpointing: model stores all activations → uses **16 GB VRAM**.\n",
        "* With checkpointing: recomputes activations → uses **\\~8 GB VRAM** but training is slightly slower.\n",
        "\n",
        "**Why important:**\n",
        "\n",
        "* Lets you train **larger batch sizes** or **higher resolutions** on limited VRAM GPUs.\n",
        "\n",
        "**If missing:**\n",
        "\n",
        "* Uses more memory → may **OOM** (Out of Memory) errors.\n",
        "* Training faster (because recomputation isn’t needed).\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Summary Table\n",
        "\n",
        "| Argument                      | What it does                                  | Why important                     | If missing                                    |\n",
        "| ----------------------------- | --------------------------------------------- | --------------------------------- | --------------------------------------------- |\n",
        "| `concepts_list`               | JSON list of concepts                         | Multi-concept training in one run | Must train each concept separately            |\n",
        "| `instance_prompt`             | Text description of your instance             | Teaches model to identify subject | Model learns generic features only            |\n",
        "| `gradient_accumulation_steps` | Accumulate gradients for effective batch size | Stability on small GPUs           | Smaller batch, unstable gradients             |\n",
        "| `lr_scheduler`                | Controls learning rate schedule               | Smooth convergence                | Could overfit / underfit                      |\n",
        "| `mixed_precision`             | Uses FP16 training                            | Reduces VRAM, speeds up           | Might OOM, slower                             |\n",
        "| `gradient_checkpointing`      | Recomputes activations to save memory         | Enables training on low VRAM      | May OOM if VRAM insufficient, faster training |\n",
        "\n",
        "---\n",
        "\n",
        "### **Practical Example**\n",
        "\n",
        "Assume:\n",
        "\n",
        "* 10 instance images of “Dave”\n",
        "* 100 class images (generic “person”)\n",
        "* GPU: 12GB\n",
        "\n",
        "```bash\n",
        "python3 train_dreambooth.py \\\n",
        "  --pretrained_model_name_or_path=\"runwayml/stable-diffusion-v1-5\" \\\n",
        "  --output_dir=\"./output_dave\" \\\n",
        "  --concepts_list=\"concepts_list.json\" \\\n",
        "  --with_prior_preservation --prior_loss_weight=1.0 \\\n",
        "  --instance_prompt=\"photo of zwx person\" \\\n",
        "  --class_prompt=\"photo of a person\" \\\n",
        "  --train_batch_size=1 \\\n",
        "  --gradient_accumulation_steps=2 \\\n",
        "  --learning_rate=5e-6 \\\n",
        "  --lr_scheduler=\"constant\" \\\n",
        "  --max_train_steps=800 \\\n",
        "  --resolution=512 \\\n",
        "  --mixed_precision=\"fp16\" \\\n",
        "  --gradient_checkpointing \\\n",
        "  --use_8bit_adam\n",
        "```\n",
        "\n",
        "**Explanation of resource savings:**\n",
        "\n",
        "* `train_batch_size=1` + `gradient_accumulation_steps=2` → effective batch = 2\n",
        "* `fp16` + `gradient_checkpointing` → fits SD training in 12GB VRAM\n",
        "* Prior preservation (`with_prior_preservation`) → avoids overfitting to 10 images\n",
        "\n",
        "---\n",
        "\n",
        "If you want, I can **draw a diagram showing how `instance_prompt`, `class_prompt`, concepts list, gradient accumulation, FP16, and checkpointing interact** during DreamBooth training — it makes the flow super clear visually.\n",
        "\n",
        "Do you want me to do that?\n"
      ],
      "metadata": {
        "id": "t7Mj8KXns16U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from natsort import natsorted\n",
        "from glob import glob\n",
        "import os\n",
        "# Use glob to find directories that look like step numbers, then sort and take the last one.\n",
        "# This assumes your training script saves checkpoints in directories named by step number.\n",
        "# Alternatively, if you know the exact step number, you can specify the directory directly.\n",
        "# weights_dir = natsorted(glob(os.path.join(output_dir, \"[0-9]*\")))[-1]\n",
        "\n",
        "# Explicitly setting the weights directory as requested by the user.\n",
        "weights_dir = \"/content/stable_diffusion_weights/zwx/800\""
      ],
      "metadata": {
        "id": "mICUS6Dis0lQ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"weight directory:\" + weights_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4sf4SV_uO-k",
        "outputId": "ea69c5e0-9c79-4b6e-9ebe-ed92ded856d8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weight directory:/content/stable_diffusion_weights/zwx/800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "def grid_img(imgs, row=1, cols=3, scale =1):\n",
        "  assert len(imgs) == row*cols\n",
        "  w, h = imgs[0].size\n",
        "  w, h = int(w*scale), int(h*scale)\n",
        "  grid = Image.new('RGB', size=(cols*w, row*h))\n",
        "  for i, img in enumerate(imgs):\n",
        "    img = img.resize((w, h), Image.LANCZOS)\n",
        "    grid.paste(img, box=(i%cols*w, i//cols*h))\n",
        "  return grid"
      ],
      "metadata": {
        "id": "L3l9flbluuM-"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# convert the weights into checkpoints :"
      ],
      "metadata": {
        "id": "kJAxxchCwE6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt_path = os.path.join(weights_dir, \"model.ckpt\")\n",
        "half_arg = \"--half\" # fp 16\n",
        "!python /content/convert_diffusers_to_original_stable_diffusion.py --model_path $weights_dir --checkpoint_path $ckpt_path $half_arg\n",
        "print(f\"  ckpt saved at {ckpt_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xfhGkdDwH7U",
        "outputId": "c9aeb0e0-4000-4a97-c767-f5dbe2813fd2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ckpt saved at /content/stable_diffusion_weights/zwx/800/model.ckpt\n"
          ]
        }
      ]
    }
  ]
}